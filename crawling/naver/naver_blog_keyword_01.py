# # Library

import requests
from bs4 import BeautifulSoup as bs

import time, os, random
import pandas as pd
import numpy as np

from selenium import webdriver
import chromedriver_autoinstaller as ca

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ## chrome driver ì„¤ì¹˜

# USB error ë©”ì„¸ì§€ ë°œìƒ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ
options = webdriver.ChromeOptions()
options.add_experimental_option("excludeSwitches", ["enable-logging"])

# í˜„ì¬ í¬ë¡¬ ë²„ì „ í™•ì¸
chrome_ver = ca.get_chrome_version().split('.')[0]

# # í¬ë¡¬ ë“œë¼ì´ë²„ í™•ì¸ ë° ì„¤ì¹˜(ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰)
# ca.install(True)

# # í˜ì´ì§€ ì ‘ì†

# url = 'https://section.blog.naver.com/BlogHome.naver?directoryNo=0&currentPage=1&groupId=0' # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í™ˆ

# ## requests í…ŒìŠ¤íŠ¸
# - í˜ì´ì§€ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€í™•ì¸
#     - ê°€ëŠ¥í•  ê²½ìš° ì¶œë ¥ : <Response [200]>

# req = requests.get(url)
# print(req)

# í•œê¸€ ê¹¨ì§ í•´ê²° ì½”ë“œ
# # html = req.content.decode('utf-8') # í•œê¸€ ê¹¨ì§ í•´ê²°
# # soup = bs(html, 'html.parser')

# soup = bs(req.text, 'html.parser')
# soup.title.text

# requestë¡œ ìˆ˜ì§‘ ì—ëŸ¬
keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
page_num = 1 # í˜ì´ì§€ ë²ˆí˜¸
keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}'
req = requests.get(keyword_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'})
print(req)
soup = bs(req.text, 'html.parser')

# area_list_search = soup.select_one('div.area_list_search')
# list_search_post = area_list_search.select('div.list_search_post')
# list_search_post[0]

# ## keyword ì…ë ¥

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
page_num = 1 # í˜ì´ì§€ ë²ˆí˜¸
tab_option = 'ê¸€'
if tab_option == 'ê¸€':
    keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}'
elif tab_option == 'ë¸”ë¡œê·¸':
    keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy=sim&keyword={keyword}'

# ## íƒ­ ì„ íƒ(ê¸€, ë¸”ë¡œê·¸)

# tab_option = 'ë¸”ë¡œê·¸'

# if tab_option == 'ê¸€':
#     page = driver.page_source
#     soup = bs(page, 'html.parser')
#     print('ê¸€')
#     print(soup.title.text)

# elif tab_option == 'ë¸”ë¡œê·¸':
#     blog_tab_xpath = '//*[@id="content"]/section/div[1]/div[1]/a[2]'
#     blog_tab = driver.find_element(By.XPATH, blog_tab_xpath)
#     blog_tab.click()
#     driver.implicitly_wait(10)

#     page = driver.page_source
#     soup = bs(page, 'html.parser')
#     print('ë¸”ë¡œê·¸')
#     print(soup.title.text)

# ## selenium ì‘ë™

# ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

driver.get(keyword_url)
driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
# driver.maximize_window() # ë¸Œë¼ìš°ì ¸ ì°½ ìµœëŒ€í™”

# keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
# keyword_input_xpath = '//*[@id="header"]/div[1]/div/div[2]/form/fieldset/div/input' # ê²€ìƒ‰ì°½ xpath
# keyword_input_box = driver.find_element(By.XPATH, keyword_input_xpath)
# keyword_input_box.send_keys(keyword)
# time.sleep(random.uniform(1, 2))
# keyword_input_box.send_keys(Keys.ENTER)
# driver.implicitly_wait(10)

# # ì •ë³´ ìˆ˜ì§‘

# ### ğŸ”§ ê¸€ ì„ íƒ
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=WEEK&orderBy=sim&startDate=2025-01-03&endDate=2025-01-10&keyword=ì•„ì´í° 16
# - rangeType ì˜µì…˜(ê¸°ê°„)
#     - ê¸°ê°„ì „ì²´ : &rangeType=ALL
#     - ìµœê·¼ 1ì£¼ : &rangeType=WEEK
#     - ìµœê·¼ 1ê°œì›” : &rangeType=MONTH
#     - ê¸°ê°„ ì…ë ¥ : &startDate=YYYY-mm-dd&endDate=YYYY-mm-dd
# 
# - orderBy ì˜µì…˜(ê´€ë ¨ë„ìˆœ, ìµœì‹ ìˆœ)
#     - ê´€ë ¨ë„ìˆœ : &orderBy=sim
#     - ìµœì‹ ìˆœ : &orderBy=recentdate

page = driver.page_source
soup = bs(page, 'html.parser')
print(soup.title.text)

area_list_search = soup.select_one('div.area_list_search')
list_search_post = area_list_search.select('div.list_search_post')

# ê¸€ ì œëª©
post_num = 0
title = list_search_post[post_num].select_one('span.title').text

# ë¸”ë¡œê·¸ ë‚´ìš©
text = list_search_post[post_num].select_one('a.text').text

# ê¸€ ì‘ì„±ì
name_author = list_search_post[post_num].select_one('em.name_author').text

# ë¸”ë¡œê·¸ ì´ë¦„
name_blog = list_search_post[post_num].select_one('span.name_blog').text

# ê¸€ ì‘ì„± ë‚ ì§œ
date = list_search_post[post_num].select_one('span.date').text

# ê¸€ ë§í¬
desc_inner = list_search_post[post_num].select_one('a.desc_inner')['href']

# ê¸€ ì œëª©
post_num = 1
title = list_search_post[post_num].select_one('span.title').text

# ë¸”ë¡œê·¸ ë‚´ìš©
text = list_search_post[post_num].select_one('a.text').text

# ê¸€ ì‘ì„±ì
name_author = list_search_post[post_num].select_one('em.name_author').text

# ë¸”ë¡œê·¸ ì´ë¦„
name_blog = list_search_post[post_num].select_one('span.name_blog').text

# ê¸€ ì‘ì„± ë‚ ì§œ
date = list_search_post[post_num].select_one('span.date').text

# ê¸€ ë§í¬
text_link = list_search_post[post_num].select_one('a.desc_inner')['href']

area_list_search = soup.select_one('div.area_list_search')
list_search_post = area_list_search.select('div.list_search_post')

title_list = []
text_list = []
name_author_list = []
name_blog_list = []
date_list = []
text_link_list = []

for post in list_search_post:
    title = post.select_one('span.title').text
    text = post.select_one('a.text').text
    name_author = post.select_one('em.name_author').text
    name_blog = post.select_one('span.name_blog').text
    date = post.select_one('span.date').text
    text_link = post.select_one('a.desc_inner')['href']
    # print(title, text, name_author, name_blog, text_link)

    title_list.append(title)
    text_list.append(text)
    name_author_list.append(name_author)
    name_blog_list.append(name_blog)
    date_list.append(date)
    text_link_list.append(text_link)
print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(text_link_list))

# ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
tab_option = 'ê¸€'

title_list = []
text_list = []
name_author_list = []
name_blog_list = []
date_list = []
post_link_list = []

for page_num in range(1, 10):
    if tab_option == 'ê¸€':
        keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}'
    elif tab_option == 'ë¸”ë¡œê·¸':
        keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy=sim&keyword={keyword}'

    driver.get(keyword_url)
    driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
    time.sleep(random.uniform(1, 3))

    page = driver.page_source
    soup = bs(page, 'html.parser')
    # print(soup.title.text)
    
    area_list_search = soup.select_one('div.area_list_search')
    list_search_post = area_list_search.select('div.list_search_post')

    for post in list_search_post:
        title = post.select_one('span.title').text
        text = post.select_one('a.text').text
        name_author = post.select_one('em.name_author').text
        name_blog = post.select_one('span.name_blog').text
        date = post.select_one('span.date').text
        post_link = post.select_one('a.desc_inner')['href']

        title_list.append(title)
        text_list.append(text)
        name_author_list.append(name_author)
        name_blog_list.append(name_blog)
        date_list.append(date)
        post_link_list.append(post_link)

print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(post_link_list))

post_dict = {
    'title': title_list,
    'text': text_list,
    'name_author': name_author_list,
    'name_blog': name_blog_list,
    'date': date_list,
    'post_link': post_link_list
}
post_df = pd.DataFrame(post_dict)

new = post_df.drop_duplicates()

# ## ğŸ”§ ë¸”ë¡œê·¸ ì„ íƒ
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Blog.naver?pageNo=1&orderBy=sim&keyword=ì•„ì´í° 16
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Blog.naver?pageNo=1&orderBy=recentdate&keyword=ì•„ì´í° 16
# - orderBy ì˜µì…˜
#     - ê´€ë ¨ë„ìˆœ : &orderBy=sim
#     - ìµœì‹ ìˆœ : &orderBy=recentdate

page_num = 1 # í˜ì´ì§€ ë²ˆí˜¸
tab_option = 'ë¸”ë¡œê·¸'
if tab_option == 'ê¸€':
    keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}'
elif tab_option == 'ë¸”ë¡œê·¸':
    keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy=sim&keyword={keyword}'

driver.get(keyword_url)
driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
# driver.maximize_window() # ë¸Œë¼ìš°ì ¸ ì°½ ìµœëŒ€í™”

page = driver.page_source
soup = bs(page, 'html.parser')
print(soup.title.text)

area_list_search = soup.select_one('div.area_list_search')
list_search_blog = area_list_search.select('div.list_search_blog')

# ë¸”ë¡œê·¸ ì œëª©
post_num = 0
text_blog = list_search_blog[post_num].select_one('em.text_blog').text

# ë¸”ë¡œê·¸ ë‚´ìš©
blog_intro = list_search_blog[post_num].select_one('p.blog_intro').text

# ê¸€ ì‘ì„±ì
name_author = list_search_blog[post_num].select_one('em.name_author').text


# ë¸”ë¡œê·¸ ë§í¬
blog_link = list_search_blog[post_num].select_one('a.name_blog')['href']



# ë¸”ë¡œê·¸ ì œëª©
post_num = 1
text_blog = list_search_blog[post_num].select_one('em.text_blog').text

# ë¸”ë¡œê·¸ ë‚´ìš©
blog_intro = list_search_blog[post_num].select_one('p.blog_intro').text

list_search_blog[post_num].select_one('p.blog_intro').text == ''

# ê¸€ ì‘ì„±ì
name_author = list_search_blog[post_num].select_one('em.name_author').text

# ë¸”ë¡œê·¸ ë§í¬
blog_link = list_search_blog[post_num].select_one('a.name_blog')['href']

# ì •ë³´ ìˆ˜ì§‘
text_blog_list = []
blog_intro_list = []
name_author_list = []
blog_link_list = []
for blog in list_search_blog:
    text_blog = blog.select_one('em.text_blog').text
    if blog.select_one('p.blog_intro').text == '':
        blog_intro = np.nan
    else:
        blog_intro = blog.select_one('p.blog_intro').text
    name_author = blog.select_one('em.name_author').text
    blog_link = blog.select_one('a.name_blog')['href']
    # print(text_blog, blog_intro, name_author, blog_link)

    text_blog_list.append(text_blog)
    blog_intro_list.append(blog_intro)
    name_author_list.append(name_author)
    blog_link_list.append(blog_link)

print(len(text_blog_list), len(blog_intro_list), len(name_author_list), len(name_blog_list))

blog_dict = {
    'text_blog': text_blog_list,
    'blog_intro': blog_intro_list,
    'name_author': name_author_list,
    'blog_link': blog_link_list
}
blog_df = pd.DataFrame(blog_dict)

# # ğŸ”§ ì¢…í•©
# - ê¸€, ë¸”ë¡œê·¸ ì˜µì…˜ ì„¤ì • ì ìš© : ê´€ë ¨ë„, ê¸°ê°„ ë“±

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
tab_option = 'ê¸€'

if tab_option == 'ê¸€':
    title_list = []
    text_list = []
    name_author_list = []
    name_blog_list = []
    date_list = []
    post_link_list = []
    for page_num in range(1, 10):
        keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}'

        driver.get(keyword_url)
        driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
        time.sleep(random.uniform(1, 3))

        page = driver.page_source
        soup = bs(page, 'html.parser')
        # print(soup.title.text)
        
        area_list_search = soup.select_one('div.area_list_search')
        list_search_post = area_list_search.select('div.list_search_post')

        for post in list_search_post:
            title = post.select_one('span.title').text
            text = post.select_one('a.text').text
            name_author = post.select_one('em.name_author').text
            name_blog = post.select_one('span.name_blog').text
            date = post.select_one('span.date').text
            post_link = post.select_one('a.desc_inner')['href']

            title_list.append(title)
            text_list.append(text)
            name_author_list.append(name_author)
            name_blog_list.append(name_blog)
            date_list.append(date)
            post_link_list.append(post_link)

    print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(post_link_list))
    
elif tab_option == 'ë¸”ë¡œê·¸':
    for page_num in range(1, 10):
        keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy=sim&keyword={keyword}'

        driver.get(keyword_url)
        driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
        time.sleep(random.uniform(1, 3))

        page = driver.page_source
        soup = bs(page, 'html.parser')
        # print(soup.title.text)

        area_list_search = soup.select_one('div.area_list_search')
        list_search_post = area_list_search.select('div.list_search_post')

        for post in list_search_post:
            title = post.select_one('span.title').text
            text = post.select_one('a.text').text
            name_author = post.select_one('em.name_author').text
            name_blog = post.select_one('span.name_blog').text
            date = post.select_one('span.date').text
            post_link = post.select_one('a.desc_inner')['href']

            title_list.append(title)
            text_list.append(text)
            name_author_list.append(name_author)
            name_blog_list.append(name_blog)
            date_list.append(date)
            post_link_list.append(post_link)

    print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(post_link_list))

