# # Library

import requests
from bs4 import BeautifulSoup as bs

import time, os, random
import pandas as pd
import numpy as np
from datetime import datetime

from selenium import webdriver
import chromedriver_autoinstaller as ca

from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ## chrome driver ì„¤ì¹˜

# USB error ë©”ì„¸ì§€ ë°œìƒ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ
options = webdriver.ChromeOptions()
options.add_experimental_option("excludeSwitches", ["enable-logging"])

# í˜„ì¬ í¬ë¡¬ ë²„ì „ í™•ì¸
chrome_ver = ca.get_chrome_version().split('.')[0]
chrome_ver

# # í¬ë¡¬ ë“œë¼ì´ë²„ í™•ì¸ ë° ì„¤ì¹˜(ì²˜ìŒ í•œë²ˆë§Œ ì‹¤í–‰)
# ca.install(True)

# # í˜ì´ì§€ ì ‘ì†

# url = 'https://section.blog.naver.com/BlogHome.naver?directoryNo=0&currentPage=1&groupId=0' # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í™ˆ

# ## requests í…ŒìŠ¤íŠ¸
# - í˜ì´ì§€ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€í™•ì¸
#     - ê°€ëŠ¥í•  ê²½ìš° ì¶œë ¥ : <Response [200]>

# req = requests.get(url)
# print(req)

# í•œê¸€ ê¹¨ì§ í•´ê²° ì½”ë“œ
# # html = req.content.decode('utf-8') # í•œê¸€ ê¹¨ì§ í•´ê²°
# # soup = bs(html, 'html.parser')

# soup = bs(req.text, 'html.parser')
# soup.title.text

# requestë¡œ ìˆ˜ì§‘ ì—ëŸ¬
keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
page_num = 1 # í˜ì´ì§€ ë²ˆí˜¸
rangetype = 'ALL' # ê²€ìƒ‰ ë²”ìœ„
orderby = 'sim' # ì •ë ¬ ìˆœì„œ

keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&keyword={keyword}'
req = requests.get(keyword_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'})
print(req)
soup = bs(req.text, 'html.parser')

# area_list_search = soup.select_one('div.area_list_search')
# list_search_post = area_list_search.select('div.list_search_post')
# list_search_post[0]

# ## keyword ì…ë ¥

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´

# ## íƒ­ ì„ íƒ(ê¸€, ë¸”ë¡œê·¸)

tab_option = 'ê¸€' # í¬ìŠ¤íŠ¸, ë¸”ë¡œê·¸ ì„ íƒ
page_num = 1 # í˜ì´ì§€ ë²ˆí˜¸
rangetype = 'ALL' # ê²€ìƒ‰ ë²”ìœ„
orderby = 'sim' # ì •ë ¬ ìˆœì„œ

if tab_option == 'ê¸€':
    keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&keyword={keyword}'
elif tab_option == 'ë¸”ë¡œê·¸':
    keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy={orderby}&keyword={keyword}'

# ## selenium ì‘ë™

# ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

driver.get(keyword_url)
driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
# driver.maximize_window() # ë¸Œë¼ìš°ì ¸ ì°½ ìµœëŒ€í™”

# keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
# keyword_input_xpath = '//*[@id="header"]/div[1]/div/div[2]/form/fieldset/div/input' # ê²€ìƒ‰ì°½ xpath
# keyword_input_box = driver.find_element(By.XPATH, keyword_input_xpath)
# keyword_input_box.send_keys(keyword)
# time.sleep(random.uniform(1, 2))
# keyword_input_box.send_keys(Keys.ENTER)
# driver.implicitly_wait(10)

# # ì •ë³´ ìˆ˜ì§‘

# ### ğŸ”§ ê¸€ ì„ íƒ
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType=ALL&orderBy=sim&keyword={keyword}
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=WEEK&orderBy=sim&startDate=2025-01-03&endDate=2025-01-10&keyword=ì•„ì´í° 16
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=PERIOD&orderBy=sim&startDate=2025-01-01&endDate=2025-01-13&keyword=ì•„ì´í° 16
# - [â­•] rangeType ì˜µì…˜(ê¸°ê°„)
#     - ê¸°ê°„ì „ì²´ : &rangeType=ALL
#     - ìµœê·¼ 1ì£¼ : &rangeType=WEEK
#     - ìµœê·¼ 1ê°œì›” : &rangeType=MONTH
#     - ê¸°ê°„ ì…ë ¥ : &rangeType=PERIOD&startDate=YYYY-mm-dd&endDate=YYYY-mm-dd
# 
# - [â­•] orderBy ì˜µì…˜(ê´€ë ¨ë„ìˆœ, ìµœì‹ ìˆœ)
#     - ê´€ë ¨ë„ìˆœ : &orderBy=sim
#     - ìµœì‹ ìˆœ : &orderBy=recentdate
# 
# - ìˆ˜ì§‘í•  í˜ì´ì§€ ì˜µì…˜ ì¶”ê°€
# - ìˆ˜ì§‘í•  í˜ì´ì§€ê°€ ì‹¤ì œ í˜ì´ì§€ ìˆ˜ë³´ë‹¤ ë§ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬

page = driver.page_source
soup = bs(page, 'html.parser')
print(soup.title.text)

area_list_search = soup.select_one('div.area_list_search')
list_search_post = area_list_search.select('div.list_search_post')
list_search_post[0]

# ê¸€ ì œëª©
post_num = 0
title = list_search_post[post_num].select_one('span.title').text
title

# ë¸”ë¡œê·¸ ë‚´ìš©
text = list_search_post[post_num].select_one('a.text').text
text

# ê¸€ ì‘ì„±ì
name_author = list_search_post[post_num].select_one('em.name_author').text
name_author

# ë¸”ë¡œê·¸ ì´ë¦„
name_blog = list_search_post[post_num].select_one('span.name_blog').text
name_blog

# ê¸€ ì‘ì„± ë‚ ì§œ
date = list_search_post[post_num].select_one('span.date').text
date

# ê¸€ ë§í¬
desc_inner = list_search_post[post_num].select_one('a.desc_inner')['href']
desc_inner

list_search_post[1]

# ê¸€ ì œëª©
post_num = 1
title = list_search_post[post_num].select_one('span.title').text
title

# ë¸”ë¡œê·¸ ë‚´ìš©
text = list_search_post[post_num].select_one('a.text').text
text

# ê¸€ ì‘ì„±ì
name_author = list_search_post[post_num].select_one('em.name_author').text
name_author

# ë¸”ë¡œê·¸ ì´ë¦„
name_blog = list_search_post[post_num].select_one('span.name_blog').text
name_blog

# ê¸€ ì‘ì„± ë‚ ì§œ
date = list_search_post[post_num].select_one('span.date').text
date

# ê¸€ ë§í¬
text_link = list_search_post[post_num].select_one('a.desc_inner')['href']
text_link

area_list_search = soup.select_one('div.area_list_search')
list_search_post = area_list_search.select('div.list_search_post')

title_list = []
text_list = []
name_author_list = []
name_blog_list = []
date_list = []
text_link_list = []

for post in list_search_post:
    title = post.select_one('span.title').text
    text = post.select_one('a.text').text
    name_author = post.select_one('em.name_author').text
    name_blog = post.select_one('span.name_blog').text
    date = post.select_one('span.date').text
    text_link = post.select_one('a.desc_inner')['href']
    # print(title, text, name_author, name_blog, text_link)

    title_list.append(title)
    text_list.append(text)
    name_author_list.append(name_author)
    name_blog_list.append(name_blog)
    date_list.append(date)
    text_link_list.append(text_link)
print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(text_link_list))

driver.quit()

# ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

start = 5
end = 3
start, end = end, start
print(start, end)

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
tab_option = 'ê¸€' # í¬ìŠ¤íŠ¸, ë¸”ë¡œê·¸ ì„ íƒ
rangetype = 'PERIOD' # ê²€ìƒ‰ ë²”ìœ„(ALL, WEEK, MONTH, PERIOD)
orderby = 'sim' # ì •ë ¬ ìˆœì„œ(sim, recentdate)

# rangetypeì´ PERIODì¸ ê²½ìš° ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì • 
startdate = '2025-01-01' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
enddate = '2025-01-10' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
current_date = datetime.today().strftime('%Y-%m-%d')
if startdate > enddate:
    startdate, enddate = enddate, startdate # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ í¬ë©´ ì¢…ë£Œì¼ë¡œ ë³€ê²½
if startdate > current_date:
    startdate, enddate = current_date, current_date # ì‹œì‘ì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½
if enddate > current_date:
    enddate = current_date # ì¢…ë£Œì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½

title_list = []
text_list = []
name_author_list = []
name_blog_list = []
date_list = []
post_link_list = []

for page_num in range(1, 10):
    if tab_option == 'ê¸€':
        if rangetype == 'PERIOD':
            keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&&startDate={startdate}&endDate={enddate}&keyword={keyword}'
        else:
            keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&keyword={keyword}'
    elif tab_option == 'ë¸”ë¡œê·¸':
        keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy={orderby}&keyword={keyword}'


    driver.get(keyword_url)
    driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
    time.sleep(random.uniform(1, 3))

    page = driver.page_source
    soup = bs(page, 'html.parser')
    # print(soup.title.text)
    
    area_list_search = soup.select_one('div.area_list_search')
    list_search_post = area_list_search.select('div.list_search_post')

    for post in list_search_post:
        title = post.select_one('span.title').text
        text = post.select_one('a.text').text
        name_author = post.select_one('em.name_author').text
        name_blog = post.select_one('span.name_blog').text
        date = post.select_one('span.date').text
        post_link = post.select_one('a.desc_inner')['href']

        title_list.append(title)
        text_list.append(text)
        name_author_list.append(name_author)
        name_blog_list.append(name_blog)
        date_list.append(date)
        post_link_list.append(post_link)

print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(post_link_list))

post_dict = {
    'title': title_list,
    'text': text_list,
    'name_author': name_author_list,
    'name_blog': name_blog_list,
    'date': date_list,
    'post_link': post_link_list
}
post_df = pd.DataFrame(post_dict)
post_df

new = post_df.drop_duplicates()
new.shape

# í˜„ì¬ ë‚ ì§œ
current_date = datetime.today().strftime('%Y%m%d')
current_date

# í˜„ì¬ ê²½ë¡œ í™•ì¸
code_path = os.getcwd().replace('\\', '/')
code_path

# ìˆ˜ì§‘í•œ íŒŒì¼ ì €ì¥í•  í´ë” ìƒì„±
crawled_folder_path = os.path.join(code_path, 'crawled_data', 'naver_blog', current_date)
os.makedirs(crawled_folder_path, exist_ok=True)

current_datetime = datetime.today().strftime('%Y%m%d_%p_%I%M%S')
current_datetime

# ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
file_path = os.path.join(crawled_folder_path, f'naver_blog_{current_datetime}.xlsx')
file_path



# ## ğŸ”§ ë¸”ë¡œê·¸ ì„ íƒ
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Blog.naver?pageNo=1&orderBy=sim&keyword=ì•„ì´í° 16
# - ì°¸ê³  ë§í¬ : https://section.blog.naver.com/Search/Blog.naver?pageNo=1&orderBy=recentdate&keyword=ì•„ì´í° 16
# - [â­•] orderBy ì˜µì…˜
#     - ê´€ë ¨ë„ìˆœ : &orderBy=sim
#     - ìµœì‹ ìˆœ : &orderBy=recentdate
# 
# - ìˆ˜ì§‘í•  í˜ì´ì§€ ì˜µì…˜ ì¶”ê°€
# - ìˆ˜ì§‘í•  í˜ì´ì§€ê°€ ì‹¤ì œ í˜ì´ì§€ ìˆ˜ë³´ë‹¤ ë§ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´
tab_option = 'ë¸”ë¡œê·¸' # í¬ìŠ¤íŠ¸, ë¸”ë¡œê·¸ ì„ íƒ
# rangetype = 'PERIOD' # ê²€ìƒ‰ ë²”ìœ„(ALL, WEEK, MONTH, PERIOD)
orderby = 'sim' # ì •ë ¬ ìˆœì„œ(sim, recentdate)

# # rangetypeì´ PERIODì¸ ê²½ìš° ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì • 
# startdate = '2025-01-01' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
# enddate = '2025-01-10' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
# current_date = datetime.today().strftime('%Y-%m-%d')
# if startdate > enddate:
#     startdate, enddate = enddate, startdate # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ í¬ë©´ ì¢…ë£Œì¼ë¡œ ë³€ê²½
# if startdate > current_date:
#     startdate, enddate = current_date, current_date # ì‹œì‘ì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½
# if enddate > current_date:
#     enddate = current_date # ì¢…ë£Œì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½

if tab_option == 'ê¸€':
    if rangetype == 'PERIOD':
        keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&&startDate={startdate}&endDate={enddate}&keyword={keyword}'
    else:
        keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&keyword={keyword}'
elif tab_option == 'ë¸”ë¡œê·¸':
    keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy={orderby}&keyword={keyword}'

keyword_url

driver.get(keyword_url)
driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
# driver.maximize_window() # ë¸Œë¼ìš°ì ¸ ì°½ ìµœëŒ€í™”

page = driver.page_source
soup = bs(page, 'html.parser')
print(soup.title.text)

area_list_search = soup.select_one('div.area_list_search')
list_search_blog = area_list_search.select('div.list_search_blog')
list_search_blog[0]

# ë¸”ë¡œê·¸ ì œëª©
post_num = 0
text_blog = list_search_blog[post_num].select_one('em.text_blog').text
text_blog

# ë¸”ë¡œê·¸ ë‚´ìš©
blog_intro = list_search_blog[post_num].select_one('p.blog_intro').text
blog_intro

# ê¸€ ì‘ì„±ì
name_author = list_search_blog[post_num].select_one('em.name_author').text
name_author

# ë¸”ë¡œê·¸ ë§í¬
blog_link = list_search_blog[post_num].select_one('a.name_blog')['href']
blog_link



# ë¸”ë¡œê·¸ ì œëª©
post_num = 1
text_blog = list_search_blog[post_num].select_one('em.text_blog').text
text_blog

# ë¸”ë¡œê·¸ ë‚´ìš©
blog_intro = list_search_blog[post_num].select_one('p.blog_intro').text
blog_intro

list_search_blog[post_num].select_one('p.blog_intro').text == ''

# ê¸€ ì‘ì„±ì
name_author = list_search_blog[post_num].select_one('em.name_author').text
name_author

# ë¸”ë¡œê·¸ ë§í¬
blog_link = list_search_blog[post_num].select_one('a.name_blog')['href']
blog_link

area_list_search = soup.select_one('div.area_list_search')
list_search_blog = area_list_search.select('div.list_search_blog')

# ì •ë³´ ìˆ˜ì§‘
text_blog_list = []
blog_intro_list = []
name_author_list = []
blog_link_list = []
for blog in list_search_blog:
    text_blog = blog.select_one('em.text_blog').text
    if blog.select_one('p.blog_intro').text == '':
        blog_intro = np.nan
    else:
        blog_intro = blog.select_one('p.blog_intro').text
    name_author = blog.select_one('em.name_author').text
    blog_link = blog.select_one('a.name_blog')['href']
    # print(text_blog, blog_intro, name_author, blog_link)

    text_blog_list.append(text_blog)
    blog_intro_list.append(blog_intro)
    name_author_list.append(name_author)
    blog_link_list.append(blog_link)

print(len(text_blog_list), len(blog_intro_list), len(name_author_list), len(name_blog_list))

blog_dict = {
    'text_blog': text_blog_list,
    'blog_intro': blog_intro_list,
    'name_author': name_author_list,
    'blog_link': blog_link_list
}
blog_df = pd.DataFrame(blog_dict)

# ## ğŸ”§ ì¢…í•©
# - ìˆ˜ì§‘í•  í˜ì´ì§€ ì˜µì…˜ ì¶”ê°€
# - ìˆ˜ì§‘í•  í˜ì´ì§€ê°€ ì‹¤ì œ í˜ì´ì§€ ìˆ˜ë³´ë‹¤ ë§ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬

keyword = 'ì•„ì´í° 16' # ê²€ìƒ‰ì–´

rangetype = 'PERIOD' # ê²€ìƒ‰ ë²”ìœ„(ALL, WEEK, MONTH, PERIOD)
orderby = 'sim' # ì •ë ¬ ìˆœì„œ(sim, recentdate)

# rangetypeì´ PERIODì¸ ê²½ìš° ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì • 
startdate = '2025-01-01' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
enddate = '2025-01-10' # í˜•ì‹: YYYY-mm-dd(ì˜ˆ. 2025-01-01)
current_date = datetime.today().strftime('%Y-%m-%d')
if startdate > enddate:
    startdate, enddate = enddate, startdate # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ í¬ë©´ ì¢…ë£Œì¼ë¡œ ë³€ê²½
if startdate > current_date:
    startdate, enddate = current_date, current_date # ì‹œì‘ì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½
if enddate > current_date:
    enddate = current_date # ì¢…ë£Œì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ í¬ë©´ í˜„ì¬ ë‚ ì§œë¡œ ë³€ê²½

# í˜„ì¬ ë‚ ì§œ
current_date = datetime.today().strftime('%Y%m%d')
# í˜„ì¬ ê²½ë¡œ í™•ì¸
code_path = os.getcwd().replace('\\', '/')
# ìˆ˜ì§‘í•œ íŒŒì¼ ì €ì¥í•  í´ë” ìƒì„±
crawled_folder_path = os.path.join(code_path, 'crawled_data', 'naver_blog', current_date)
os.makedirs(crawled_folder_path, exist_ok=True)
# ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
current_datetime = datetime.today().strftime('%Y%m%d_%p_%I%M%S')
file_path = os.path.join(crawled_folder_path, f'naver_blog_{current_datetime}.xlsx')

# ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

sheet_name_list = ['ê¸€', 'ë¸”ë¡œê·¸']
with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
    for tab_option in sheet_name_list:
        if tab_option == 'ê¸€':
            title_list = []
            text_list = []
            name_author_list = []
            name_blog_list = []
            date_list = []
            post_link_list = []
            for page_num in range(1, 10):
                if rangetype == 'PERIOD':
                    keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&&startDate={startdate}&endDate={enddate}&keyword={keyword}'
                else:
                    keyword_url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page_num}&rangeType={rangetype}&orderBy={orderby}&keyword={keyword}'

                driver.get(keyword_url)
                driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
                time.sleep(random.uniform(1, 3))

                page = driver.page_source
                soup = bs(page, 'html.parser')
                # print(soup.title.text)
                
                area_list_search = soup.select_one('div.area_list_search')
                list_search_post = area_list_search.select('div.list_search_post')

                for post in list_search_post:
                    title = post.select_one('span.title').text
                    text = post.select_one('a.text').text
                    name_author = post.select_one('em.name_author').text
                    name_blog = post.select_one('span.name_blog').text
                    date = post.select_one('span.date').text
                    post_link = post.select_one('a.desc_inner')['href']

                    title_list.append(title)
                    text_list.append(text)
                    name_author_list.append(name_author)
                    name_blog_list.append(name_blog)
                    date_list.append(date)
                    post_link_list.append(post_link)

            # print(len(title_list), len(text_list), len(name_author_list), len(name_blog_list), len(date_list), len(post_link_list))

            # ë°ì´í„° í”„ë ˆì„ ìƒì„±
            post_dict = {
                'title': title_list,
                'text': text_list,
                'name_author': name_author_list,
                'name_blog': name_blog_list,
                'date': date_list,
                'post_link': post_link_list
            }
            post_df = pd.DataFrame(post_dict)
            post_df.to_excel(writer, sheet_name=tab_option, index=False)
            print(post_df.shape)

        elif tab_option == 'ë¸”ë¡œê·¸':
            text_blog_list = []
            blog_intro_list = []
            name_author_list = []
            blog_link_list = []
            for page_num in range(1, 10):
                keyword_url = f'https://section.blog.naver.com/Search/Blog.naver?pageNo={page_num}&orderBy={orderby}&keyword={keyword}'

                driver.get(keyword_url)
                driver.implicitly_wait(10) # í˜ì´ì§€ ë¡œë“œ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ë§Œ ë¡œë“œ ë˜ëŠ” ìˆœê°„ ì¢…ë£Œ
                time.sleep(random.uniform(1, 3))

                page = driver.page_source
                soup = bs(page, 'html.parser')
                # print(soup.title.text)

                area_list_search = soup.select_one('div.area_list_search')
                list_search_blog = area_list_search.select('div.list_search_blog')

                for blog in list_search_blog:
                    text_blog = blog.select_one('em.text_blog').text
                    if blog.select_one('p.blog_intro').text == '':
                        blog_intro = np.nan
                    else:
                        blog_intro = blog.select_one('p.blog_intro').text
                    name_author = blog.select_one('em.name_author').text
                    blog_link = blog.select_one('a.name_blog')['href']
                    # print(text_blog, blog_intro, name_author, blog_link)

                    text_blog_list.append(text_blog)
                    blog_intro_list.append(blog_intro)
                    name_author_list.append(name_author)
                    blog_link_list.append(blog_link)

                # print(len(text_blog_list), len(blog_intro_list), len(name_author_list), len(name_blog_list))
            blog_dict = {
                'text_blog': text_blog_list,
                'blog_intro': blog_intro_list,
                'name_author': name_author_list,
                'blog_link': blog_link_list
            }
            blog_df = pd.DataFrame(blog_dict)
            blog_df.to_excel(writer, sheet_name=tab_option, index=False)
            print(blog_df.shape)

    print('ì €ì¥ íŒŒì¼ ê²½ë¡œ :', file_path)
    print('ì €ì¥ì™„ë£Œ')

driver.quit()

# # END